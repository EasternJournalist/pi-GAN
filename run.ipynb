{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save_dir = '/output'\n",
    "ckpt_dir = '/output'\n",
    "data_dir = '/data/52WangRuicheng/FFHQ64/image64_rescale'\n",
    "cache_dir = '/data/52WangRuicheng/FFHQ64pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle\n",
      "Transform\n",
      "[69994/69994]\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder=None, image_size=64, transparent:bool=False, aug_prob:bool=0., exts = ['jpg', 'jpeg', 'png']):\n",
    "        super(ImageDataset, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        if os.path.exists('/data/52WangRuicheng/FFHQ64pkl/all_imgs.pkl'):\n",
    "            print('Load pickle')\n",
    "            with open('/data/52WangRuicheng/FFHQ64pkl/all_imgs.pkl', 'rb') as f:\n",
    "                self.all_imgs = pickle.load(f)\n",
    "        else:\n",
    "            print('Get Paths...')\n",
    "            self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
    "            assert len(self.paths) > 0, f'No images were found in {folder} for training'\n",
    "    \n",
    "            self.all_imgs = []\n",
    "            print('Read images...')\n",
    "            for i, path in enumerate(self.paths):\n",
    "                self.all_imgs.append(Image.open(path))\n",
    "                print(f'[{i + 1:>5d}/{len(self.paths):>5d}]', end='\\r')\n",
    "            print()\n",
    "            print('Save pickle...')\n",
    "            with open('/data/52WangRuicheng/FFHQ64pkl/all_imgs.pkl', 'wb') as f:\n",
    "                pickle.dump(self.all_imgs, f, 1)\n",
    "        self.img_arr = None\n",
    "        \n",
    "        self.create_transform(image_size)\n",
    "        \n",
    "    def create_transform(self, image_size):\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(image_size),\n",
    "            T.CenterCrop(image_size),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.img_arr = []\n",
    "        print('Transform')\n",
    "        for i, img in enumerate(self.all_imgs):\n",
    "            self.img_arr.append(self.transform(img))\n",
    "            print(f'[{i + 1:>5d}/{len(self.all_imgs):>5d}]', end='\\r')\n",
    "        self.img_arr = torch.stack(self.img_arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.img_arr[index]\n",
    "\n",
    "dataset = ImageDataset(data_dir, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_cam_poses(std_yaw, std_pitch, num:int) -> torch.Tensor:\n",
    "    'return list of 4x4 matrices'\n",
    "    cam2world = []\n",
    "    for i in range(num):\n",
    "        yaw = torch.randn(1) * std_yaw\n",
    "        pitch = torch.randn(1) * std_pitch\n",
    "        r = 1.\n",
    "        cam2world.append(torch.tensor([ \n",
    "            [torch.cos(yaw),    -torch.sin(pitch) * torch.sin(yaw), torch.cos(pitch) * torch.sin(yaw),  r * torch.cos(pitch) * torch.sin(yaw)], \n",
    "            [0.,                torch.cos(pitch),                   torch.sin(pitch),                   r * torch.sin(pitch)],\n",
    "            [-torch.sin(yaw),   -torch.sin(pitch) * torch.cos(yaw), torch.cos(pitch) * torch.cos(yaw),  r *torch.cos(pitch) * torch.cos(yaw)],\n",
    "            [0.,                0.,                                 0.,                                 1.]]))\n",
    "    return torch.stack(cam2world)\n",
    "\n",
    "def get_cam_poses(yaws, pitchs, rs) ->torch.Tensor:\n",
    "    'return list of 4x4 matrices'\n",
    "    cam2world = []\n",
    "    for yaw, pitch, r in zip(list(yaws), list(pitchs), list(rs)):\n",
    "        cam2world.append(torch.tensor([ \n",
    "            [torch.cos(yaw),    -torch.sin(pitch) * torch.sin(yaw), torch.cos(pitch) * torch.sin(yaw),  r * torch.cos(pitch) * torch.sin(yaw)], \n",
    "            [0.,                torch.cos(pitch),                   torch.sin(pitch),                   r * torch.sin(pitch)],\n",
    "            [-torch.sin(yaw),   -torch.sin(pitch) * torch.cos(yaw), torch.cos(pitch) * torch.cos(yaw),  r *torch.cos(pitch) * torch.cos(yaw)],\n",
    "            [0.,                0.,                                 0.,                                 1.]]))\n",
    "    return torch.stack(cam2world)\n",
    "\n",
    "def get_ray_bundle(height:int, width:int, fov_y:float, cam2world:torch.Tensor):\n",
    "    'return shape (height, width, 3), (height, width, 3)'\n",
    "    ii, jj = torch.meshgrid(torch.arange(height).to(cam2world), torch.arange(width).to(cam2world))\n",
    "\n",
    "    directions = torch.stack([(jj - width * 0.5) / height * 2. * fov_y, -(ii - height * 0.5) / height * 2. * fov_y, -torch.ones_like(ii)], dim=-1)\n",
    "    ray_directions = torch.sum(directions[..., None, :] * cam2world[:3, :3], dim=-1)\n",
    "    ray_origins = cam2world[:3, -1].expand(ray_directions.shape)\n",
    "    return ray_origins, ray_directions\n",
    "\n",
    "def compute_query_points_from_rays(\n",
    "    ray_origins:torch.Tensor,\n",
    "    ray_directions:torch.Tensor,\n",
    "    near_thresh:float,\n",
    "    far_thresh:float,\n",
    "    num_samples:int,\n",
    "    randomize:bool=True,\n",
    "    depth_values=None,\n",
    "):\n",
    "    'return shape (height, width, num_samples, 3), (height, width, num_sample)'\n",
    "    if depth_values is None:\n",
    "        depth_values = torch.linspace(near_thresh, far_thresh, num_samples).to(ray_origins).expand(list(ray_directions.shape[:-1]) + [num_samples])\n",
    "        if randomize is True:\n",
    "            noise_shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "            depth_values = depth_values + torch.rand(noise_shape).to(ray_origins) * (far_thresh - near_thresh) / num_samples\n",
    "    else:\n",
    "        num_samples = depth_values.shape[-1]\n",
    "    query_points = ray_origins[..., None, :] + ray_directions[..., None, :] * depth_values[..., :, None]\n",
    "    query_dirs = ray_directions[..., None, :].repeat([1] * (ray_directions.dim() - 1) + [num_samples, 1])\n",
    "    return query_points, query_dirs, depth_values\n",
    "\n",
    "def sample_importance(\n",
    "    depth_values:torch.Tensor,\n",
    "    weights:torch.Tensor,\n",
    "    num_samples:int,\n",
    "    randomize = True\n",
    "):\n",
    "    ''' return depth sampled according to weights. shape (height, width, num_samples)'''\n",
    "    weights = weights + 1e-5 \n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "\n",
    "    if randomize:\n",
    "        u = torch.rand(list(cdf.shape[:-1]) + [num_samples]).to(weights)\n",
    "    else:\n",
    "        u = torch.linspace(0., 1., num_samples).to(weights).expand(list(weights.shape[:-1]) + [num_samples])\n",
    "\n",
    "    inds = torch.searchsorted(cdf.detach(), u, right=False).clamp(0, cdf.shape[-1] - 1)\n",
    "    \n",
    "    rng = torch.cat([depth_values[...,1:], depth_values[..., -1:]], dim=-1) - torch.cat([depth_values[..., :1], depth_values[..., :-1]], dim=-1)\n",
    "    \n",
    "    new_depth_values = torch.gather(depth_values, -1, inds)\n",
    "    depth_rand_rng = torch.gather(rng, -1, inds)\n",
    "    new_depth_values = new_depth_values + (torch.rand_like(new_depth_values) - 0.5) * depth_rand_rng\n",
    "    \n",
    "    return new_depth_values\n",
    "\n",
    "def render_volume_weight(\n",
    "    radiance_field:torch.Tensor,                    # (height, width, num_samples, 4)\n",
    "    ray_origins:torch.Tensor,                       # (height, width, 3)\n",
    "    depth_values:torch.Tensor,                      # (height, width, num_samples, 3)\n",
    "    dense_aug:float=1.0\n",
    "):\n",
    "    sigma_a = F.softplus(radiance_field[..., 3] * dense_aug)        # (height, width, num_samples)\n",
    "\n",
    "    one_e_10 = torch.tensor([1e3], dtype=ray_origins.dtype, device=ray_origins.device)\n",
    "    dists = torch.cat([depth_values[..., 1:] - depth_values[..., :-1], one_e_10.expand(depth_values[..., :1].shape)], dim=-1)\n",
    "    alpha = sigma_a * dists\n",
    "    weights = (1. - torch.exp(-alpha)) * torch.exp(-(torch.cumsum(alpha, dim=-1) - alpha))\n",
    "\n",
    "    return weights            # (height, width, num_samples)\n",
    "\n",
    "def render_volume(\n",
    "    radiance_field:torch.Tensor,                    # (height, width, num_samples, 4)\n",
    "    ray_origins:torch.Tensor,                       # (height, width, 3)\n",
    "    depth_values:torch.Tensor,                      # (height, width, \n",
    "    dense_aug:float=1\n",
    "):\n",
    "    sigma_a = F.softplus(radiance_field[..., 3] * dense_aug)        # (height, width, num_samples)\n",
    "    rgb = torch.sigmoid(radiance_field[..., :3])    # (height, width, num_samples, 3)\n",
    "\n",
    "    one_e_10 = torch.tensor([1e3], dtype=ray_origins.dtype, device=ray_origins.device)\n",
    "    dists = torch.cat([depth_values[..., 1:] - depth_values[..., :-1], one_e_10.expand(depth_values[..., :1].shape)], dim=-1)\n",
    "    alpha = sigma_a * dists\n",
    "    weights = (1. - torch.exp(-alpha)) * torch.exp(-(torch.cumsum(alpha, dim=-1) - alpha))\n",
    "\n",
    "    rgb_map = (weights[..., None] * rgb).sum(dim=-2)\n",
    "    depth_map = (weights * depth_values).sum(dim=-1)\n",
    "    acc_map = weights.sum(-1)\n",
    "\n",
    "    return rgb_map, depth_map.detach_(), acc_map            # (height, width, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from functools import partial\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad as torch_grad\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from functools import partial\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad as torch_grad\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# losses\n",
    "def gradient_penalty(images, output) -> torch.Tensor:\n",
    "    batch_size, device = images.shape[0], images.device\n",
    "    gradients = torch_grad(outputs=output, inputs=images,\n",
    "                           grad_outputs=torch.ones(output.size(), device=device),\n",
    "                           create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "    l2 = ((gradients.norm(2, dim = 1) - 1) ** 2).mean()\n",
    "    return l2\n",
    "\n",
    "# FiLM SIREN y = sin(gamma * (w x + b) + beta)\n",
    "class FiLMSIREN(nn.Module):\n",
    "    def __init__(self, in_features:int, out_features:int, omega_0:float=30., is_first:bool=False, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.is_first = is_first\n",
    "        self.omega_0 = omega_0\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            b = math.sqrt(6. / self.in_features) if self.is_first else math.sqrt(6. / self.in_features) / self.omega_0\n",
    "            self.linear.weight.uniform_(-b, b)\n",
    "\n",
    "    def forward(self, x, gamma=None, beta=None):\n",
    "        out =  self.linear(x)\n",
    "        # FiLM modulation\n",
    "        if gamma is not None:\n",
    "            out = out * gamma\n",
    "        if beta is not None:\n",
    "            out = out + beta\n",
    "\n",
    "        out = torch.sin(self.omega_0 * out)\n",
    "        return out\n",
    "\n",
    "# Equalized Learning rate initialzation for linear (in PG-GAN). Default for nn.Linear is Kaming initialization.\n",
    "class EqualizedLinear(nn.Linear):\n",
    "    def __init__(self, in_features:int, out_features:int, bias:bool=True, nonlinearity:str='leaky_relu', *args, **kwargs):\n",
    "        self.nonlinearity = nonlinearity\n",
    "        super(EqualizedLinear, self).__init__(in_features=in_features, out_features=out_features, bias=bias, *args, **kwargs)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = nn.init.calculate_gain(self.nonlinearity) / math.sqrt(self.in_features)\n",
    "        nn.init.normal_(self.weight, mean=0., std=std)\n",
    "        bound = 1 / math.sqrt(self.in_features)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping network\n",
    "class MappingNetwork(nn.Module):    \n",
    "    def __init__(self, dim_in:int=128, dim_hidden:int=256, num_hidden:int=3, dim_out=[256 for i in range(8)]+[128]):\n",
    "        super(MappingNetwork, self).__init__()\n",
    "\n",
    "        mlp_layers = [EqualizedLinear(dim_in, dim_hidden), nn.LeakyReLU(negative_slope=0.2)]\n",
    "        for i in range(num_hidden - 1):\n",
    "            mlp_layers.append(EqualizedLinear(dim_hidden, dim_hidden))\n",
    "            mlp_layers.append(nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        self.to_gammas = nn.ModuleList([\n",
    "            #nn.Sequential(\n",
    "            #    nn.Linear(dim_hidden, dim_hidden),\n",
    "            #    nn.LeakyReLU(negative_slope=0.2),\n",
    "                nn.Linear(dim_hidden, d)\n",
    "            #)\n",
    "             for d in dim_out\n",
    "        ])\n",
    "        self.to_betas = [] # nn.ModuleList([nn.Linear(dim_hidden, d) for d in dim_out[:4]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        gammas = [l(x) * 2. for l in self.to_gammas]\n",
    "        betas = [l(x) * (0.4 * 0.5**i) for i, l in enumerate(self.to_betas)] + [None] * (len(self.to_gammas) - len(self.to_betas))\n",
    "        \n",
    "        return gammas, betas\n",
    "\n",
    "\n",
    "# generator\n",
    "class SIRENGenerator(nn.Module):\n",
    "    def __init__(self, *, dim_latent:int=128, dim_hidden:int=256, siren_num_layers:int=8):\n",
    "        super().__init__()\n",
    "        dim_x = 3\n",
    "        dim_d = 3\n",
    "\n",
    "        self.mapping = MappingNetwork(dim_in=dim_latent, dim_hidden=dim_hidden, dim_out=[dim_hidden] * siren_num_layers)\n",
    "\n",
    "        self.filmsiren_series = nn.ModuleList(\n",
    "            [FiLMSIREN(in_features=dim_x, out_features=dim_hidden, is_first=True)] + [FiLMSIREN(in_features=dim_hidden, out_features=dim_hidden) for i in range(siren_num_layers - 1)]\n",
    "        )\n",
    "        self.to_alpha = nn.Linear(dim_hidden, 1)\n",
    "\n",
    "        self.to_rgb_siren = FiLMSIREN(in_features=dim_hidden + dim_d, out_features=dim_hidden // 2)\n",
    "        self.to_rgb_linear = nn.Linear(dim_hidden // 2, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, latent_z, coords_x, views_d, batch_size:int=8192):\n",
    "        latent_z = latent_z.view(1, -1)\n",
    "\n",
    "        gammas, betas = self.mapping(latent_z)\n",
    "          \n",
    "        outs = []\n",
    "        for x, d in zip(coords_x.split(batch_size), views_d.split(batch_size)):\n",
    "            for i, l in enumerate(self.filmsiren_series):\n",
    "                x = l(x, gammas[i], betas[i])\n",
    "            alpha = self.to_alpha(x)\n",
    "            \n",
    "            x = torch.cat([x, d], dim=1)\n",
    "            x = self.to_rgb_siren(x, None, None)\n",
    "            rgb = self.to_rgb_linear(x)\n",
    "\n",
    "            out = torch.cat((rgb, alpha), dim=-1)\n",
    "            outs.append(out)\n",
    "            \n",
    "        return torch.cat(outs)\n",
    "        \n",
    "\n",
    "class ImageGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        dim_latent,\n",
    "        dim_hidden,\n",
    "        siren_num_layers,\n",
    "        device_ids\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_latent = dim_latent\n",
    "        self.image_size = image_size\n",
    "        self.device_ids = device_ids\n",
    "        \n",
    "        self.nerf_model = SIRENGenerator(\n",
    "            dim_latent=dim_latent,\n",
    "            dim_hidden=dim_hidden,\n",
    "            siren_num_layers=siren_num_layers\n",
    "        )\n",
    "        \n",
    "    def make_parallel(self):\n",
    "        if not isinstance(self.nerf_model, nn.DataParallel):\n",
    "            self.nerf_model = nn.DataParallel(self.nerf_model, device_ids=self.device_ids).cuda()\n",
    "    \n",
    "    def no_parallel(self):\n",
    "        if isinstance(self.nerf_model, nn.DataParallel):\n",
    "            self.nerf_model = self.nerf_model.module\n",
    "\n",
    "    def set_image_size(self, image_size):\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def forward(self, latents:torch.Tensor, camera_poses:torch.Tensor, samples_per_ray=32, with_importance=True, dense_aug=1.0):\n",
    "        image_size = self.image_size\n",
    "\n",
    "        generated_images = self.get_image_from_nerf_model(\n",
    "            latents,\n",
    "            camera_poses,\n",
    "            image_size,\n",
    "            image_size,\n",
    "            samples_per_ray=samples_per_ray,\n",
    "            with_importance=with_importance,\n",
    "            dense_aug=dense_aug\n",
    "        )\n",
    "\n",
    "        return generated_images\n",
    "    \n",
    "    def get_image_from_nerf_model(\n",
    "        self,\n",
    "        latents:torch.Tensor,\n",
    "        cam2world:torch.Tensor,\n",
    "        height:int,\n",
    "        width:int,\n",
    "        fov_y=0.2,\n",
    "        near_thresh:float=0.7,\n",
    "        far_thresh:float=1.3,\n",
    "        samples_per_ray=128,\n",
    "        with_importance:bool=True,\n",
    "        dense_aug=1.0\n",
    "    ):\n",
    "        assert latents.shape[0] == cam2world.shape[0]\n",
    "\n",
    "        images = []\n",
    "        depth_images = []\n",
    "        \n",
    "        for latent, c2w in zip(latents.unbind(dim=0), cam2world.unbind(0)):   # for each (latent, c2w), generate an image\n",
    "            c2w = c2w.cuda() \n",
    "            ray_origins, ray_directions = get_ray_bundle(height, width, fov_y, c2w)\n",
    "            \n",
    "            if with_importance:\n",
    "                # uniform sample 1/2\n",
    "                query_points, query_dirs, depth_values = compute_query_points_from_rays(ray_origins, ray_directions, near_thresh, far_thresh, samples_per_ray // 2, False, None)\n",
    "                flattened_query_points = query_points.reshape((-1, 3))\n",
    "                flattened_query_dirs = query_dirs.reshape((-1, 3))\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    radiance_field_flattened = self.nerf_model(latent, flattened_query_points, flattened_query_dirs).detach()\n",
    "                unflattened_shape = list(query_points.shape[:-1]) + [4]\n",
    "                radiance_field = torch.reshape(radiance_field_flattened, unflattened_shape)\n",
    "                weights = render_volume_weight(radiance_field, ray_origins, depth_values, dense_aug)\n",
    "\n",
    "                # importance sample 1/2\n",
    "                new_depth_values = sample_importance(depth_values, weights,  samples_per_ray // 2, randomize=True)\n",
    "                depth_values = torch.cat([depth_values, new_depth_values], dim=-1).sort()[0]\n",
    "                query_points, query_dirs, new_depth_values = compute_query_points_from_rays(ray_origins, ray_directions, near_thresh, far_thresh, samples_per_ray, False, depth_values)\n",
    "            else:\n",
    "                query_points, query_dirs, depth_values = compute_query_points_from_rays(ray_origins, ray_directions, near_thresh, far_thresh, samples_per_ray, True, None)\n",
    "            \n",
    "            flattened_query_points = query_points.reshape((-1, 3))\n",
    "            flattened_query_dirs = query_dirs.reshape((-1, 3))\n",
    "            \n",
    "            radiance_field_flattened = self.nerf_model(latent, flattened_query_points, flattened_query_dirs)\n",
    "            unflattened_shape = list(query_points.shape[:-1]) + [4]\n",
    "            radiance_field = torch.reshape(radiance_field_flattened, unflattened_shape)\n",
    "            rgb_predicted, depth_predicted, _ = render_volume(radiance_field, ray_origins, depth_values, dense_aug)\n",
    "            image = rgb_predicted.permute((2, 0, 1))    # (h, w, c) -> (c, h, w)\n",
    "            \n",
    "            images.append(image)\n",
    "            depth_images.append(depth_predicted)\n",
    "\n",
    "        return torch.stack(images), torch.stack(depth_images)\n",
    "\n",
    "# CoordConv\n",
    "class CoordConv2D(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, kernel_size:int=3, stride:int=1, padding:int=1, with_r:bool=False):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channels\n",
    "        self.with_r = with_r\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels + (2 if not with_r else 3), out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, input_tensor:torch.Tensor):\n",
    "        batch_size, _, y_dim, x_dim = input_tensor.size()\n",
    "\n",
    "        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n",
    "        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (x_dim - 1) * 2. - 1.\n",
    "        yy_channel = yy_channel.float() / (y_dim - 1) * 2. - 1.\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1)\n",
    "        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "        x = torch.cat([input_tensor, xx_channel.type_as(input_tensor), yy_channel.type_as(input_tensor)], dim=1)\n",
    "\n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel.type_as(input_tensor) - 0.5, 2) + torch.pow(yy_channel.type_as(input_tensor) - 0.5, 2))\n",
    "            x = torch.cat([x, rr], dim=1)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.res = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2)\n",
    "        self.net = nn.Sequential(\n",
    "            CoordConv2D(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            CoordConv2D(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.pooling = nn.AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.res(x)\n",
    "        x = self.net(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x + res\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, init_resolution:int=16, final_resolution:int=64):\n",
    "        super().__init__()\n",
    "        assert math.log2(init_resolution).is_integer() and math.log2(final_resolution).is_integer(), 'resolution must be a power of 2'\n",
    "\n",
    "        self.init_resolution = init_resolution\n",
    "        self.final_resolution = final_resolution\n",
    "\n",
    "        log2_init_res = int(math.log2(init_resolution))\n",
    "        log2_final_res = int(math.log2(final_resolution))\n",
    "        \n",
    "        self.resolutions = [final_resolution // 2**i for i in range(0, log2_final_res - 1)] # Input resolution of each block\n",
    "        chans = [min(64*2**i, 400) for i in range(0, log2_final_res - 1)]                   # Output channels of each block\n",
    "\n",
    "        self.conv_blocks = nn.ModuleList(\n",
    "            [DiscriminatorBlock(in_channels=(chans[i - 1] if i > 0 else chans[0]), out_channels=chans[i]) for i in range(0, len(chans))]\n",
    "        )\n",
    "        \n",
    "        # adaper_blocks[i] before conv_blocks[i]\n",
    "        self.adapter_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                CoordConv2D(3, chans[i - 1] if i > 0 else chans[0]), \n",
    "                nn.LeakyReLU(negative_slope=0.2)\n",
    "            ) for i in range(0, log2_final_res - log2_init_res + 1)\n",
    "        ])\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_channels=400, out_channels=1, kernel_size=2)\n",
    "        \n",
    "        self.fadein = False\n",
    "        self.fadein_iters = 6000\n",
    "        self.alpha = 0.\n",
    "        self.iterations = 0\n",
    "        self.cur_resolution = init_resolution\n",
    "\n",
    "    def set_resolution(self, new_resolution, fadein=True):\n",
    "        if new_resolution >= self.final_resolution:\n",
    "            return\n",
    "        self.fadein = fadein\n",
    "        self.alpha = 0.\n",
    "        self.iterations = 0\n",
    "        self.cur_resolution = new_resolution\n",
    "\n",
    "    def update_iter_(self):\n",
    "        if not self.fadein:\n",
    "            return\n",
    "        self.iterations += 1\n",
    "        self.alpha += (1 / self.fadein_iters)\n",
    "        if self.alpha >= 1.:\n",
    "            self.fadein = False\n",
    "            self.alpha = 1.\n",
    "\n",
    "    def forward(self, img:torch.Tensor):\n",
    "        for i in range(len(self.conv_blocks)):\n",
    "            if self.cur_resolution < self.resolutions[i]:\n",
    "                continue\n",
    "            if self.cur_resolution == self.resolutions[i]:\n",
    "                x = self.adapter_blocks[i](img)\n",
    "\n",
    "            if self.cur_resolution // 2 == self.resolutions[i] and self.fadein:\n",
    "                x_down = F.avg_pool2d(input=img, kernel_size=2) # F.interpolate(img, scale_factor = 0.5, mode=)\n",
    "                x = x * self.alpha + self.adapter_blocks[i](x_down) * (1. - self.alpha)\n",
    "\n",
    "            x = self.conv_blocks[i](x)\n",
    "        \n",
    "        out = self.final_conv(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n",
    "# pi-GAN class\n",
    "class piGAN:\n",
    "    dataloader:DataLoader\n",
    "\n",
    "    def __init__(self, dataset, device_ids=[0, 1, 2, 3]):\n",
    "        self.image_size = 64\n",
    "        self.G = nn.DataParallel(ImageGenerator(\n",
    "            image_size=16,\n",
    "            dim_latent=128,\n",
    "            dim_hidden=256,\n",
    "            siren_num_layers=8,\n",
    "            device_ids=device_ids\n",
    "        ), device_ids=device_ids).cuda()\n",
    "\n",
    "        self.D = Discriminator(\n",
    "            init_resolution=16,\n",
    "            final_resolution=self.image_size\n",
    "        )\n",
    "        self.D = nn.DataParallel(self.D, device_ids=device_ids).cuda()\n",
    "\n",
    "        self.optim_G = Adam(self.G.parameters(), betas=(0, 0.9), lr=5e-5)\n",
    "        self.optim_D = Adam(self.D.parameters(), betas=(0, 0.9), lr=4e-4)\n",
    "        \n",
    "        self.lr_scheduler_G =lr_scheduler.ExponentialLR(self.optim_G, gamma=0.2**(1./30000))\n",
    "        self.lr_scheduler_D =lr_scheduler.ExponentialLR(self.optim_D, gamma=0.25**(1./30000))\n",
    "\n",
    "        self.loss_D = 0.\n",
    "        self.loss_G = 0.\n",
    "        self.loss_gp = 0.\n",
    "\n",
    "        self.batch_size_D = 128\n",
    "        self.batch_size_G = 64\n",
    "        self.with_gradient_penalty = True\n",
    "\n",
    "        self.iterations = 1\n",
    "        self.grow_iters = 10000\n",
    "\n",
    "        self.dataset = dataset \n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.batch_size_D, shuffle=True)\n",
    "        \n",
    "        self.cur_resolution = 16\n",
    "        self.dense_aug = 1.\n",
    "\n",
    "    def train(self, num_iters):\n",
    "        self.dataloader_cycle = cycle(self.dataloader)\n",
    "        \n",
    "        for i in range(num_iters):\n",
    "            if i % 100 == 0:\n",
    "                tot_loss_D = 0.\n",
    "                tot_loss_G = 0.\n",
    "                tot_loss_gp = 0.\n",
    "            i_mod = i % 100 + 1\n",
    "            \n",
    "            self.train_step()\n",
    "            self.lr_scheduler_G.step()\n",
    "            self.lr_scheduler_D.step()\n",
    "            \n",
    "            tot_loss_G += self.loss_G\n",
    "            tot_loss_D += self.loss_D\n",
    "            tot_loss_gp += self.loss_gp\n",
    "            \n",
    "            print(f'[Iter {self.iterations:>6d}] G loss:{tot_loss_G / i_mod:>5f}, D loss:{tot_loss_D / i_mod:>5f}, gp loss:{tot_loss_gp / i_mod:>5f}', end='\\r')\n",
    "            \n",
    "            if self.iterations % 100 == 0:\n",
    "                print()\n",
    "                \n",
    "            if self.iterations % 100 == 0:\n",
    "                print('[Test]')\n",
    "                self.test_imgs()\n",
    "                self.test_video(traj='circle')\n",
    "                self.test_video(traj='straight')\n",
    "                \n",
    "            \n",
    "            if self.iterations % 1000 == 0:\n",
    "                print('[Save check point]')\n",
    "                self.save_ckpt(os.path.join(ckpt_dir, f'{str(self.iterations).zfill(6)}.pth'))\n",
    "\n",
    "            self.iterations += 1\n",
    "\n",
    "    def train_step(self):\n",
    "        self.dense_aug = max(1., (self.iterations - 39000) / 500)\n",
    "        if self.iterations % self.grow_iters == 0 and self.cur_resolution < self.image_size:\n",
    "            print(f'Resolution Grow to {self.cur_resolution * 2}')\n",
    "            self.cur_resolution *= 2\n",
    "            if self.iterations != 0:\n",
    "                self.D.module.set_resolution(self.cur_resolution)\n",
    "\n",
    "            self.G.module.set_image_size(self.cur_resolution)\n",
    "            self.dataset.create_transform(self.cur_resolution)\n",
    "            self.batch_size_G = 16384 // (self.cur_resolution * self.cur_resolution)\n",
    "            \n",
    "        # Train Discriminator\n",
    "        self.D.train()\n",
    "        \n",
    "        tiny_steps = 1\n",
    "        self.loss_D = 0.\n",
    "        i = 0\n",
    "        while i == 0 or (i < tiny_steps and self.loss_D > self.loss_G):\n",
    "            i += 1\n",
    "        \n",
    "            real_imgs = next(self.dataloader_cycle)\n",
    "            real_imgs = real_imgs.cuda().requires_grad_()\n",
    "            real_imgs_D_out = self.D(real_imgs)\n",
    "            with torch.no_grad():\n",
    "                rand_latents = torch.randn(self.batch_size_D, self.G.module.dim_latent).cuda()\n",
    "                cam2world = sample_cam_poses(0.3, 0.15, self.batch_size_D).cuda()\n",
    "                fake_imgs, _ = self.G(rand_latents, cam2world, 32, self.iterations>25000, self.dense_aug)\n",
    "            fake_imgs.detach_()\n",
    "            \n",
    "            fake_imgs_D_out = self.D(fake_imgs)\n",
    "            \n",
    "            loss = torch.mean(F.softplus(fake_imgs_D_out)) + torch.mean(F.softplus(-real_imgs_D_out))\n",
    "            \n",
    "            self.loss_D += loss.item() / tiny_steps\n",
    "            if self.with_gradient_penalty:\n",
    "                gp = gradient_penalty(real_imgs, real_imgs_D_out)\n",
    "                self.loss_gp = gp.item()\n",
    "                loss = loss + 10. * gp\n",
    "            \n",
    "            self.optim_D.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        tiny_steps = 1\n",
    "        \n",
    "        self.G.train()\n",
    "        i = 0\n",
    "        while i == 0 or (i < tiny_steps and self.loss_G > self.loss_D * 2.):\n",
    "            i += 1\n",
    "            #print(f'G [{i:5d}/{tiny_steps:5d}]', end='\\r')\n",
    "            rand_latents = torch.randn(self.batch_size_G // 2, self.G.module.dim_latent).cuda().repeat((2, 1))\n",
    "            cam2world = sample_cam_poses(0.3, 0.15, self.batch_size_G).cuda()\n",
    "            fake_imgs, _ = self.G(rand_latents, cam2world, 32, self.iterations>25000, self.dense_aug)\n",
    "            \n",
    "            loss = torch.mean(F.softplus(-self.D(fake_imgs)))\n",
    "            \n",
    "            self.loss_G = loss.item()\n",
    "            \n",
    "            self.optim_G.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim_G.step()\n",
    "            \n",
    "        self.D.module.update_iter_()\n",
    "\n",
    "    def test_imgs(self, n_row:int=4, n_col:int=4, title:str=None):\n",
    "        if title is None:\n",
    "            title = f'iter_{str(self.iterations).zfill(6)}'\n",
    "        '''4 x 4 = 16 images of currenct resolution'''\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            rand_latents = torch.randn(n_row, self.G.module.dim_latent).cuda()\n",
    "            rand_latents = rand_latents[:, None, :].repeat((1, n_col, 1)).reshape(n_row * n_col, -1)\n",
    "            cam2world = get_cam_poses(torch.linspace(-0.45, 0.45, n_col), torch.zeros(n_col), torch.ones(n_col)).cuda()\n",
    "            cam2world = cam2world.repeat((n_row, 1, 1))\n",
    "            fake_imgs, depth_imgs = self.G(rand_latents, cam2world, 128, True, 10.)\n",
    "        \n",
    "        depth_imgs = torch.exp(- 3. * depth_imgs + 2.)[:, None, :, :]\n",
    "        \n",
    "        torchvision.utils.save_image(fake_imgs, os.path.join(test_save_dir, f'{title}.png'), nrow=n_col)\n",
    "        torchvision.utils.save_image(depth_imgs, os.path.join(test_save_dir, f'{title}_depth.png'), nrow=n_col)\n",
    "    \n",
    "    def test_video(self, traj:str='circle', title:str=None):\n",
    "        if title is None:\n",
    "            title = f'iter_{str(self.iterations).zfill(6)}_{traj}'\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            rand_latents = torch.randn(1, self.G.module.dim_latent).cuda()\n",
    "            rand_latents = rand_latents[:, :].repeat((128, 1))\n",
    "            \n",
    "            if traj == 'circle':\n",
    "                theta = torch.linspace(0, 6.28, 128)\n",
    "                cam2world = get_cam_poses(0.3 * torch.cos(theta), 0.3 * torch.sin(theta), torch.ones(128)).cuda()\n",
    "            elif traj == 'straight':\n",
    "                cam2world = get_cam_poses(torch.linspace(-0.6, 0.6, 128), torch.zeros(128), torch.ones(128)).cuda()\n",
    "                \n",
    "            fake_imgs, _ = self.G(rand_latents, cam2world, 64, True, 10.)\n",
    "            \n",
    "        fake_imgs = (fake_imgs.permute((0, 2, 3, 1)).cpu().numpy().clip(0, 1) * 255).astype(np.uint8)\n",
    "        imageio.mimwrite(os.path.join(test_save_dir, f'{title}.mp4'), fake_imgs, fps=30, quality=8) \n",
    "    \n",
    "    def save_ckpt(self, path):\n",
    "        # self.G.no_parallel()\n",
    "        \n",
    "        state = {\n",
    "            'G':self.G.module.state_dict(), \n",
    "            'D':self.D.module.state_dict(),\n",
    "            'optim_G':self.optim_G.state_dict(),\n",
    "            'optim_D':self.optim_D.state_dict(),\n",
    "            'lr_scheduler_G':self.lr_scheduler_G.state_dict(),\n",
    "            'lr_scheduler_D':self.lr_scheduler_D.state_dict(),\n",
    "            'iterations':self.iterations,\n",
    "            \n",
    "            'cur_resolution':self.cur_resolution,\n",
    "            'fadein':self.D.module.fadein,\n",
    "            'fadein_alpha':self.D.module.alpha,\n",
    "            'fadein_iterations':self.D.module.iterations,\n",
    "            }\n",
    "        torch.save(state, path)\n",
    "        \n",
    "        # self.G.make_parallel()\n",
    "\n",
    "    def load_ckpt(self, path):\n",
    "        # self.G.no_parallel()\n",
    "        \n",
    "        state = torch.load(path)\n",
    "        print('load done')\n",
    "        self.G.module.load_state_dict(state['G'])\n",
    "        self.D.module.load_state_dict(state['D'])\n",
    "        self.optim_G.load_state_dict(state['optim_G'])\n",
    "        self.optim_D.load_state_dict(state['optim_D'])\n",
    "        self.lr_scheduler_G.load_state_dict(state['lr_scheduler_G'])\n",
    "        self.lr_scheduler_D.load_state_dict(state['lr_scheduler_D'])\n",
    "        self.iterations = state['iterations']\n",
    "        \n",
    "        self.cur_resolution = state['cur_resolution'] if 'cur_resolution' in state.keys() else 64\n",
    "        self.D.module.cur_resolution = self.cur_resolution\n",
    "        self.D.module.fadein = state['fadein'] if 'fadein' in state.keys() else False\n",
    "        self.D.module.alpha = state['fadein_alpha'] if 'fadein_alpha' in state.keys() else 0.\n",
    "        self.D.module.iterations = state['fadein_iterations'] if 'fadein_iterations' in state.keys() else 0\n",
    "        \n",
    "        \n",
    "        self.G.module.set_image_size(self.cur_resolution)\n",
    "        self.dataset.create_transform(self.cur_resolution)\n",
    "        self.batch_size_G = 16384 // (self.cur_resolution * self.cur_resolution)\n",
    "        \n",
    "        # self.G.make_parallel()\n",
    "\n",
    "    def reset_optimizer(self):\n",
    "        self.optim_G = Adam(self.G.parameters(), betas=(0, 0.9), lr=0.5e-5)\n",
    "        self.optim_D = Adam(self.D.parameters(), betas=(0, 0.9), lr=0.5e-4)\n",
    "        \n",
    "        self.lr_scheduler_G =lr_scheduler.ExponentialLR(self.optim_G, gamma=0.2**(1./20000))\n",
    "        self.lr_scheduler_D =lr_scheduler.ExponentialLR(self.optim_D, gamma=0.25**(1./20000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n",
      "Transform\n",
      "[69994/69994]\r"
     ]
    }
   ],
   "source": [
    "model = piGAN(dataset)\n",
    "model.load_ckpt('/model/52WangRuicheng/piGAN_ckpt40000_nobias/ckpt040000.pth')\n",
    "model.reset_optimizer()\n",
    "os.makedirs('/output/test', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_imgs(4, 4, 'softplus_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_video('straight', 'geo_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter  40000] G loss:2.113070, D loss:0.623099, gp loss:0.002009\n",
      "[Test]\n",
      "[Save check point]\n",
      "[Iter  40100] G loss:1.385327, D loss:0.618386, gp loss:0.002098\n",
      "[Test]\n",
      "[Iter  40200] G loss:1.604851, D loss:0.625586, gp loss:0.001828\n",
      "[Test]\n",
      "[Iter  40300] G loss:0.944841, D loss:0.600411, gp loss:0.002491\n",
      "[Test]\n",
      "[Iter  40400] G loss:1.032014, D loss:0.648286, gp loss:0.002431\n",
      "[Test]\n",
      "[Iter  40500] G loss:1.561739, D loss:0.634215, gp loss:0.002445\n",
      "[Test]\n",
      "[Iter  40600] G loss:1.122288, D loss:0.643881, gp loss:0.003305\n",
      "[Test]\n",
      "[Iter  40700] G loss:1.468914, D loss:0.645529, gp loss:0.002920\n",
      "[Test]\n",
      "[Iter  40794] G loss:1.546522, D loss:0.638435, gp loss:0.002619\r"
     ]
    }
   ],
   "source": [
    "model.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
